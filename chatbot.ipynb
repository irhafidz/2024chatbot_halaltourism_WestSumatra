{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irhafidz/2024chatbot_halaltourism_WestSumatra/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-tBmrEScJuv"
      },
      "source": [
        "# **Install and Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qtv3tWsgVk1d"
      },
      "outputs": [],
      "source": [
        "# For Google Colab\n",
        "!pip install torch\n",
        "!pip install accelerate\n",
        "!pip install torchinfo\n",
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DODV9ykzv4_I"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "from nltk import download\n",
        "from transformers import Trainer\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TrainingArguments\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5Wz1N5-NJp9"
      },
      "outputs": [],
      "source": [
        "# Download resource dan setting\n",
        "download('stopwords')\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWFHYNylcmRd"
      },
      "source": [
        "# **Prepare Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNs9zZ73bZKN"
      },
      "source": [
        "## **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTAqr0qVUmqq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/chatlist.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvoqdfZGbH-9"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTPlgkt8ZWtg"
      },
      "outputs": [],
      "source": [
        "# Cek jumlah variasi pertanyaan per label\n",
        "print(f\"Jumlah variasi pertanyaan per {df['labelx'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRr57_DUbQMb"
      },
      "outputs": [],
      "source": [
        "# Cek distribusi label\n",
        "print(f\"Distribusi {df['labelx'].value_counts(normalize = True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sz3LTAvbVkI"
      },
      "outputs": [],
      "source": [
        "# Dapatkan semua label unik\n",
        "categories = np.unique(list(df['labelx']))\n",
        "categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bd322m1brmL"
      },
      "source": [
        "## **Cleaning Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1MCuIQabOfl"
      },
      "outputs": [],
      "source": [
        "# Encode labelx menjadi angka\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['labelx'])\n",
        "print('Hasil label encoding:')\n",
        "for i in range(len(le.classes_)):\n",
        "  print(f\"{i} = {le.classes_[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ_t5-klVQQY"
      },
      "outputs": [],
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stop = stopwords.words('indonesian')\n",
        "\n",
        "def prepare_question(text: str) -> str:\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[,](?!\\s)', ', ', text)  # Add spasi pada koma tanpa spasi\n",
        "  text = re.sub(r'[.](?!\\s)', '. ', text)  # Add spasi pada titik tanpa spasi\n",
        "  text = text.replace('\\\\t', ' ').replace('\\\\n', ' ').replace('\\\\u', ' ')  # Hapus tab, new line, , dll\n",
        "  text = text.encode('ascii', 'replace').decode('ascii')  # Hapus karakter non ASCII (emoticon, chinese word, dll)\n",
        "  text = re.sub(r\"(?i)(?:https?:\\/\\/)?(?:www\\.)?(?:[a-zA-Z0-9-.]+)(?:\\.[a-zA-Z]{2,6})(?:\\/[^\\s\\r\\n]*)?\", \"\", text)  # Hapus URL\n",
        "  text = re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\.\\-\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)  # Ubah tanda baca ke spasi\n",
        "  text = text.strip()  # Hapus whitespace di depan/belakang teks\n",
        "  text = re.sub('\\s+', ' ', text)  # Hapus double++ spasi\n",
        "  text = re.sub('\\s+(?=\\.)', '', text)  # Hapus spasi sebelum titik\n",
        "  text = re.sub(r'\\.{2,}', r'\\.', text)  # Hapus titik++\n",
        "  text = ' '.join([word for word in text.split() if word not in (stop)])  # Hapus stopwords\n",
        "  return stemmer.stem(text)  # Return hasil stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He24W4HELvZT"
      },
      "outputs": [],
      "source": [
        "df_shuffle = df.sample(frac=1, random_state=42)\n",
        "df_shuffle['stem'] = df_shuffle['text'].progress_apply(prepare_question)\n",
        "df_shuffle.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6fzGcfkb9OW"
      },
      "source": [
        "## **Split Training and Validation Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii5B-ruhqwkA"
      },
      "outputs": [],
      "source": [
        "# Menggunakan seluruh data untuk training\n",
        "dataset_text = list(df_shuffle['stem'])\n",
        "dataset_labels = list(df_shuffle['label'])\n",
        "\n",
        "# Split train_text menjadi 80% training set dan 20% validation set\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    dataset_text, dataset_labels, random_state=42, test_size=0.2\n",
        ")\n",
        "\n",
        "print(f\"Data training   : {len(train_texts)}\")\n",
        "print(f\"Data validation : {len(val_texts)}\")\n",
        "print(f\"Total data      : {len(train_texts) + len(val_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b6WY_PyatGh"
      },
      "outputs": [],
      "source": [
        "# Save Dataset\n",
        "df_shuffle.to_csv('data/clean.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sedhI9MzKQm"
      },
      "source": [
        "# **Retraining BERT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC14ii84vcyM"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-multilingual-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHS0iFP7qTp8"
      },
      "outputs": [],
      "source": [
        "# Cek apakah GPU tersedia\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running code akan menggunakan \\\"{device}\\\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VGVSC9vY5te8"
      },
      "outputs": [],
      "source": [
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input teks\n",
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "\n",
        "# Mendefinisikan PyTorch datasets\n",
        "class PyTorchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Menyiapkan dataset untuk PyTorch\n",
        "train_dataset = PyTorchDataset(train_encodings, train_labels)\n",
        "val_dataset = PyTorchDataset(val_encodings, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBTpKIbvhOGl"
      },
      "source": [
        "## **Fine-Tune Pre-Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHRBkb6FsDpI"
      },
      "outputs": [],
      "source": [
        "# Fine-tune pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzPYuDh-hS9r"
      },
      "source": [
        "## **Save Fine-Tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLECSwM1sFNl"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "output_dir = \"./bert-base-multilingual-uncased-halal-tourism\"\n",
        "model.save_pretrained(output_dir)\n",
        "print (\"Model \", output_dir, \"telah disimpan ....\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t2MBBsJpLPO"
      },
      "source": [
        "# **Chatbot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4VOh7sujFfW"
      },
      "source": [
        "## **Load Tag-Answser**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krjdWjnHuRmF"
      },
      "outputs": [],
      "source": [
        "# Menggunakan dictionary merepresentasikan daftar tag-answer sebagai file JSON\n",
        "with open('data/answer.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snvMjziDi3Zk"
      },
      "outputs": [],
      "source": [
        "# Ekstrak tags dari intents\n",
        "tags = [intent[\"tag\"] for intent in data[\"intents\"]]\n",
        "categories = np.unique(tags)\n",
        "for category in categories:\n",
        "  print(category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2UqtiwjKp7"
      },
      "source": [
        "## **Load BERT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTLywy2euRmF"
      },
      "outputs": [],
      "source": [
        "# Load BERT tokenizer\n",
        "model_name = 'bert-base-multilingual-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model_name = \"./bert-base-multilingual-uncased-halal-tourism\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2VEYeDcuRmF"
      },
      "outputs": [],
      "source": [
        "# Cek apakah GPU tersedia\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running code akan menggunakan \\\"{device}\\\"\")\n",
        "\n",
        "# Push model ke device\n",
        "model = model.to(device)\n",
        "\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZWktKvEnY3F"
      },
      "source": [
        "## **Predict Intent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHq-mesAkU2M"
      },
      "outputs": [],
      "source": [
        "def get_prediction(str):\n",
        "  example_text = re.sub(r'[^a-zA-Z ]+', '', str)\n",
        "  inputs = tokenizer(example_text, padding=True, truncation=True, return_tensors='pt').to(device)  # Tokenize input text\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)                                 # Perform inference\n",
        "  logits = outputs.logits                                     # Get predicted logits\n",
        "  probs = torch.softmax(logits, dim=-1)\t                      # Convert logits to probabilities\n",
        "  predicted_label_index = torch.argmax(probs, dim=-1).item()  # Get predicted label (index of the maximum probability)\n",
        "  predicted_label = categories[predicted_label_index]         # Get corresponding label name\n",
        "  return predicted_label\n",
        "\n",
        "def get_response(message):\n",
        "\tstart = time.time()\n",
        "\tintent = get_prediction(message)\n",
        "\tfor i in data['intents']:\n",
        "\t\tif i[\"tag\"] == intent:\n",
        "\t\t\tresult = random.choice(i[\"responses\"])\n",
        "\t\t\tbreak\n",
        "\tend = time.time()\n",
        "\tinterval = end - start\n",
        "\treturn \"Intent: \"+ intent + '\\n' + \"Response: \" + result, interval, intent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHm2ccL9iduA"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('data/testing.csv')\n",
        "\n",
        "print(f\"Ada {len(df_test.columns.to_list())} kolom, yaitu {df_test.columns.to_list()}\")\n",
        "print(f\"Ada {df_test.index.size} untuk testing\\n\")\n",
        "print(df_test['labelx'].value_counts(), \"\\n\")\n",
        "\n",
        "df_test['stem'] = df_test['text'].apply(prepare_question)\n",
        "df_test = df_test.sample(frac=1, random_state=42)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVGWbxsbrgUK"
      },
      "outputs": [],
      "source": [
        "time_list = list()\n",
        "intent_list = list()\n",
        "\n",
        "for index, row in df_test.iterrows():\n",
        "  answer, interval, intent = get_response(row['stem'])\n",
        "  intent_list.append(intent)\n",
        "  time_list.append(interval)\n",
        "  print(f\"Question: {row['text']}\")\n",
        "  # print(f\"Prepared Question : {row['stem']}\")\n",
        "  # print(f\"Real Intent : {row['labelx']}\")\n",
        "  print(answer)\n",
        "  print()\n",
        "\n",
        "print(\"Selesai...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-p2ndwSn5XV"
      },
      "source": [
        "## **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSGGHySb8Lwl"
      },
      "outputs": [],
      "source": [
        "for i in range(len(df_test)):\n",
        "  print(f\"{round(time_list[i], 5)} detik => {df_test.loc[i, 'text']}\")\n",
        "\n",
        "print()\n",
        "print(f\"Waktu tercepat = {min(time_list)}\")\n",
        "print(f\"Waktu terlama  = {max(time_list)}\")\n",
        "print(f\"Rata-rata      = {np.mean(time_list)} detik\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6QZli9_8lih"
      },
      "outputs": [],
      "source": [
        "print(classification_report(df_test.loc[:, 'labelx'], intent_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeeJH0fBLvZm"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(df_test.loc[:, 'labelx'], intent_list))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}